---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app vllm
spec:
  interval: 60m
  chart:
    spec:
      chart: vllm-stack
      version: 0.0.2
      sourceRef:
        kind: HelmRepository
        name: vllm-stack
        namespace: flux-system
  maxHistory: 3
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    routerSpec:
      enableRouter: false
    servingEngineSpec:
      modelSpec:
        - name: "opt125m"
          # repository: "lmcache/vllm-openai"
          repository: "quay.io/rhn_support_amasolov/vllm-cpu-only"
          tag: "latest"
          modelURL: "facebook/opt-125m"
          replicaCount: 1
          requestCPU: 3
          requestMemory: "16Gi"
          requestGPU: 0
          pvcStorage: "10Gi"

          vllmConfig:
            extraArgs: ["--api-key",  "token-abc123", "--chat-template",  "./examples/template_chatml.jinja"]
            enableChunkedPrefill: true
            enablePrefixCaching: true
            dtype: "bfloat16"

          env:
            - name: VLLM_RPC_TIMEOUT
              value: "1800000"
            - name: VLLM_USE_V1
              value: "1"

