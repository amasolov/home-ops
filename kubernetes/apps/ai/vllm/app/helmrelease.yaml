---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app vllm
spec:
  interval: 60m
  chart:
    spec:
      chart: vllm-stack
      version: 0.0.3
      sourceRef:
        kind: HelmRepository
        name: vllm-stack
        namespace: flux-system
  maxHistory: 3
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    routerSpec:
      enableRouter: false
      replicaCount: 0
    servingEngineSpec:
      runtimeClassName: ""
      modelSpec:
        - name: "nm-llama-31-8b"
          # repository: "lmcache/vllm-openai"
          repository: "quay.io/rhn_support_amasolov/vllm-cpu-only"
          tag: "latest"
          modelURL: "neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8"
          replicaCount: 1
          requestCPU: 2
          requestMemory: "16Gi"
          requestGPU: 0
          pvcStorage: "50Gi"
          vllmConfig:
            extraArgs: ["--max-model-len", "4096", "--api-key",  "token-abc123", "--chat-template",  "./examples/template_chatml.jinja"]
            enableChunkedPrefill: false
            enablePrefixCaching: false

          env:
            - name: VLLM_RPC_TIMEOUT
              value: "1800000"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-secret
                  key: HF_TOKEN

